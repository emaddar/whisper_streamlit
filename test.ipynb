{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python code that aims to extract text from a YouTube video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'my_functions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# !pip install -U openai-whisper\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# !sudo apt install ffmpeg\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# !pip install git+https://github.com/openai/whisper.git \u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# !pip install setuptools-rust\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmy_functions\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'my_functions'"
     ]
    }
   ],
   "source": [
    "# !pip install -U openai-whisper\n",
    "# !sudo apt install ffmpeg\n",
    "# !pip install git+https://github.com/openai/whisper.git \n",
    "# !pip install setuptools-rust\n",
    "from my_functions import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if there are more than 3 directories starts with Folder, if yes, delete it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed directory: /home/apprenant/Documents/perso/whisper_streamlit/Folder_20230704_110034\n",
      "Removed directory: /home/apprenant/Documents/perso/whisper_streamlit/Folder_20230704_110129\n",
      "Removed directory: /home/apprenant/Documents/perso/whisper_streamlit/Folder_20230704_102951\n",
      "Removed directory: /home/apprenant/Documents/perso/whisper_streamlit/Folder_20230704_105718\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Get the current directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Get a list of all directories in the current directory\n",
    "directories = [name for name in os.listdir(current_directory) if os.path.isdir(os.path.join(current_directory, name))]\n",
    "\n",
    "# Filter directories that start with \"My_Folder_\"\n",
    "matching_directories = [name for name in directories if name.startswith(\"Folder_\")]\n",
    "\n",
    "# Check if there are more than 3 matching directories\n",
    "if len(matching_directories) > 3:\n",
    "    # Remove the matching directories\n",
    "    for directory in matching_directories:\n",
    "        directory_path = os.path.join(current_directory, directory)\n",
    "        shutil.rmtree(directory_path)\n",
    "        print(f\"Removed directory: {directory_path}\")\n",
    "else:\n",
    "    print(\"There are not enough matching directories to remove.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new folder with a name corresponding to the current hour, and inside this folder, create three subfolders named \"mp4,\" \"mp3,\" and \"text.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Get the current date and time\n",
    "now = datetime.now()\n",
    "\n",
    "# Create a new folder with the current hour\n",
    "current_folder = f\"Folder_{now.strftime('%Y%m%d_%H%M%S')}\"\n",
    "os.makedirs(current_folder)\n",
    "\n",
    "# Create the subdirectories within the current folder\n",
    "mp4_directory = os.path.join(current_folder, 'media', 'mp4')\n",
    "mp3_directory = os.path.join(current_folder, 'media', 'mp3')\n",
    "txt_directory = os.path.join(current_folder, 'media', 'txt')\n",
    "\n",
    "# Create the subdirectories\n",
    "os.makedirs(mp4_directory)\n",
    "os.makedirs(mp3_directory)\n",
    "os.makedirs(txt_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # Specify the paths of the directories to delete files from\n",
    "# mp4_directory = 'medias/mp4'\n",
    "# mp3_directory = 'medias/mp3'\n",
    "# txt_directory = 'medias/txt'\n",
    "\n",
    "# # Function to delete all files in a directory\n",
    "# def delete_files_in_directory(directory):\n",
    "#     file_list = os.listdir(directory)\n",
    "#     for file_name in file_list:\n",
    "#         file_path = os.path.join(directory, file_name)\n",
    "#         if os.path.isfile(file_path):\n",
    "#             os.remove(file_path)\n",
    "\n",
    "# # Delete all files in the mp4 directory\n",
    "# delete_files_in_directory(mp4_directory)\n",
    "\n",
    "# # Delete all files in the mp3 directory\n",
    "# delete_files_in_directory(mp3_directory)\n",
    "\n",
    "# # Delete all files in the txt directory\n",
    "# delete_files_in_directory(txt_directory)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download a YouTube video from any provided link and save it in the 'mp4' directory of the newly created folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "video_url = \"https://www.youtube.com/watch?v=JlptP2bpWD8&ab_channel=SpeechesWithBeautifullyAnimatedSubtitles\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Animated Subtitles | Steve Jobs \"Stay Hungry, Stay Foolish!\" (Beautiful Subtitle To Learn English)',\n",
       " 'length': 853,\n",
       " 'thumbnail_url': 'https://i.ytimg.com/vi/JlptP2bpWD8/hq720.jpg',\n",
       " 'description': None,\n",
       " 'views': 172706,\n",
       " 'rating': None,\n",
       " 'age_restricted': False,\n",
       " 'video_id': 'JlptP2bpWD8',\n",
       " 'author': 'Speeches With Beautifully Animated Subtitles',\n",
       " 'publish_date': datetime.datetime(2019, 1, 6, 0, 0),\n",
       " 'captions': {},\n",
       " 'keywords': [],\n",
       " 'channel_url': 'https://www.youtube.com/channel/UCCBR08dMtsbQn9qKxNMzZBA',\n",
       " 'channel_id': 'UCCBR08dMtsbQn9qKxNMzZBA'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from myfunctions.my_functions import *\n",
    "\n",
    "info = get_video_info(video_url)\n",
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please waite\n",
      "                                                           \r"
     ]
    }
   ],
   "source": [
    "\n",
    "from pytube import YouTube\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    video_extension = download_youtube(video_url, mp4_directory)\n",
    "\n",
    "except:\n",
    "    print(\"please waite\")\n",
    "    video_extension= download_youtube1(video_url, mp4_directory)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename the downloaded video to 'video.mp4'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all files in the directory\n",
    "files = os.listdir(mp4_directory)\n",
    "\n",
    "# Iterate over each file and rename it\n",
    "for filename in files:\n",
    "    # Construct the current file path\n",
    "    current_path = os.path.join(mp4_directory, filename)\n",
    "    \n",
    "    # Split the current filename and extension\n",
    "    name, extension = os.path.splitext(filename)\n",
    "    \n",
    "    # Construct the new filename\n",
    "    new_name = 'video' + extension\n",
    "    \n",
    "    # Construct the new file path\n",
    "    new_path = os.path.join(mp4_directory, new_name)\n",
    "    \n",
    "    # Rename the file\n",
    "    os.rename(current_path, new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration : 84.0\n"
     ]
    }
   ],
   "source": [
    "duration = with_opencv(f\"{mp4_directory}/video.{video_extension}\")\n",
    "print(f\"Duration : {duration/60}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the video file named 'video.mp4' to an MP3 audio file and save it in the 'mp3/my_audio.mp3' directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "MoviePy error: the file Folder_20230704_110507/media/mp4/video.webm could not be found!\nPlease check that you entered the correct path.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmoviepy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meditor\u001b[39;00m \u001b[39mimport\u001b[39;00m VideoFileClip\n\u001b[1;32m      3\u001b[0m \u001b[39m# # Specify the output path for the converted MP3 file\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# mp3_path = 'medias/mp3/my_audio.mp3'\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[39m# Load the video file\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m video \u001b[39m=\u001b[39m VideoFileClip(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mmp4_directory\u001b[39m}\u001b[39;49;00m\u001b[39m/video.\u001b[39;49m\u001b[39m{\u001b[39;49;00mvideo_extension\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      9\u001b[0m \u001b[39m# Extract the audio from the video file\u001b[39;00m\n\u001b[1;32m     10\u001b[0m audio \u001b[39m=\u001b[39m video\u001b[39m.\u001b[39maudio\n",
      "File \u001b[0;32m~/miniconda3/envs/env_analyse/lib/python3.10/site-packages/moviepy/video/io/VideoFileClip.py:88\u001b[0m, in \u001b[0;36mVideoFileClip.__init__\u001b[0;34m(self, filename, has_mask, audio, audio_buffersize, target_resolution, resize_algorithm, audio_fps, audio_nbytes, verbose, fps_source)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39m# Make a reader\u001b[39;00m\n\u001b[1;32m     87\u001b[0m pix_fmt \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrgba\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m has_mask \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mrgb24\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 88\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreader \u001b[39m=\u001b[39m FFMPEG_VideoReader(filename, pix_fmt\u001b[39m=\u001b[39;49mpix_fmt,\n\u001b[1;32m     89\u001b[0m                                  target_resolution\u001b[39m=\u001b[39;49mtarget_resolution,\n\u001b[1;32m     90\u001b[0m                                  resize_algo\u001b[39m=\u001b[39;49mresize_algorithm,\n\u001b[1;32m     91\u001b[0m                                  fps_source\u001b[39m=\u001b[39;49mfps_source)\n\u001b[1;32m     93\u001b[0m \u001b[39m# Make some of the reader's attributes accessible from the clip\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mduration \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreader\u001b[39m.\u001b[39mduration\n",
      "File \u001b[0;32m~/miniconda3/envs/env_analyse/lib/python3.10/site-packages/moviepy/video/io/ffmpeg_reader.py:35\u001b[0m, in \u001b[0;36mFFMPEG_VideoReader.__init__\u001b[0;34m(self, filename, print_infos, bufsize, pix_fmt, check_duration, target_resolution, resize_algo, fps_source)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilename \u001b[39m=\u001b[39m filename\n\u001b[1;32m     34\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproc \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m infos \u001b[39m=\u001b[39m ffmpeg_parse_infos(filename, print_infos, check_duration,\n\u001b[1;32m     36\u001b[0m                            fps_source)\n\u001b[1;32m     37\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfps \u001b[39m=\u001b[39m infos[\u001b[39m'\u001b[39m\u001b[39mvideo_fps\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     38\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize \u001b[39m=\u001b[39m infos[\u001b[39m'\u001b[39m\u001b[39mvideo_size\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/env_analyse/lib/python3.10/site-packages/moviepy/video/io/ffmpeg_reader.py:270\u001b[0m, in \u001b[0;36mffmpeg_parse_infos\u001b[0;34m(filename, print_infos, check_duration, fps_source)\u001b[0m\n\u001b[1;32m    268\u001b[0m lines \u001b[39m=\u001b[39m infos\u001b[39m.\u001b[39msplitlines()\n\u001b[1;32m    269\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mNo such file or directory\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m lines[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]:\n\u001b[0;32m--> 270\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m((\u001b[39m\"\u001b[39m\u001b[39mMoviePy error: the file \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m could not be found!\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    271\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mPlease check that you entered the correct \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    272\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mpath.\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m%\u001b[39mfilename)\n\u001b[1;32m    274\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n\u001b[1;32m    277\u001b[0m \u001b[39m# get duration (in seconds)\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: MoviePy error: the file Folder_20230704_110507/media/mp4/video.webm could not be found!\nPlease check that you entered the correct path."
     ]
    }
   ],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "# # Specify the output path for the converted MP3 file\n",
    "# mp3_path = 'medias/mp3/my_audio.mp3'\n",
    "\n",
    "# Load the video file\n",
    "video = VideoFileClip(f\"{mp4_directory}/video.{video_extension}\")\n",
    "\n",
    "# Extract the audio from the video file\n",
    "audio = video.audio\n",
    "\n",
    "# Save the audio as an MP3 file\n",
    "audio.write_audiofile(f\"{mp3_directory}/my_audio.mp3\", codec='mp3')\n",
    "\n",
    "# Close the video and audio files\n",
    "video.close()\n",
    "audio.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transcribe the audio file using the tiny whisper model. The resulting transcribed text is stored in the 'result' variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " When I was young, there was an amazing publication called The Whole Earth Catalog, which was one of the Bibles of my generation. It was created by a fellow named Stuart Brandt, not far from here in Menlo Park, and he brought it to life with his poetic touch. This was in the late 60s before personal computers and desktop publishing, so it was all made with typewriters, scissors, and Polarite cameras. It was sort of like Google and paperback form 35 years before Google came along. It was idealistic, overflowing with neat tools, and great notions. Stuart and his team put out several issues of the whole Earth Catalog, and then when it had run its course, they put out a final issue. It was the mid 1970s and I was your age. On the back cover of their final issue was a photograph of an early morning country road. The kind you might find yourself hitchhiking on if you were so adventurous. Beneath it were the words, stay hungry, stay foolish. It was their farewell message as they signed off. Stay hungry, stay foolish. And I have always wished that for myself. And now, as you graduate to begin a new, I wish that for you. Stay hungry, stay foolish. Thank you all very much.\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"tiny\")\n",
    "result = model.transcribe(\"Folder_20230704_110732/media/mp3/my_audio.mp3\")\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the text to the file 'output.txt' located in the 'text' directory within the previously created folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to Folder_20230623_230136/media/txt/output.txt\n"
     ]
    }
   ],
   "source": [
    "txt_path = f\"{txt_directory}/output.txt\" \n",
    "\n",
    "# Open the file in write mode\n",
    "with open(txt_path, 'w') as file:\n",
    "    # Write the data to the file\n",
    "    file.write(result['text'])\n",
    "\n",
    "print(\"Data saved to\", txt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in Folder_20230620_010206/media/mp3/segment_1.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      " Ceci est un réseau de neurones artificiel. Un des algorithmes d'intelligence artificiel les plus sophistiqués au monde. Aller gên inspiré du fonctionnement des neurones biologiques, c'est algorique mais capable d'apprendre à réaliser n'importe quel tâche. Conduurez du voiture, jouez aux écheques, entretenir une conversation ou encore reconnaître et classer des images telles que ces chiffres que vous voyez en ce moment à l'écran. Dans cette série du vidéo, je vais vous montrer comment créer vos mêmes ce genre d'algorithme en vous expliquant en de façon simple et ludique. Tous les échoices et concepts mathématiques qui se cache derrière l'intelligence artificiel. Je vais vous apprendre à programmer des réseaux de neurones entièrement à la main, avec Nubay, mais aussi grâce au libérer Tendurflow et Keras pour qu'à la fin de ces vidéos, vous souhaitez capables de créer. Vous même, une application de vision par ordinateur comme celle que vous voyez ici. Je m'appelle Gium Sincirge, je suis d'état-saintiste, en Angleterre et je vous souhaite la bienvenue dans cette formation gratuite sur le diplône. ... Bon, dans cette première vidéo, nous allons commencer en Dubseur en voyant ensemble ce qu'est le diplône ing, qu'elle est sa place dans le monde de l'intelligence artificiel et comment fonctionne les réseaux de neurones artificiels. Mais avant tout, il y a peut-être des novices qui regardent cette vidéo, c'est pourquoi je vous propose de revoir rapidement les bases du machin learning, histoire de mettre tout le monde au même niveau. Le machin learning est un domaine de l'intelligence artificiel.\n",
      "MoviePy - Writing audio in Folder_20230620_010206/media/mp3/segment_2.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      " artificiel qui consiste à programmer une machine pour que ça s'y apprenna réaliser des tâches en étudiants des exemples de ces dernières. T'importe vu mathématiques, ces exemples sont représentés par des données que la machine utilise pour développer un modèle. Par exemple, une fonction du type f2x et gale ax plus b. Le peu de du jeu en machine learning, c'est de trouver les paramètres a et b qui donne le meilleur modèle possible. C'est-à-dire le modèle qui s'a juste le mieux a no-donné. Pour cela, on programme dans la machine un algorithme d'optimisation qui va venir tester différentes valeurs de a et b jusqu'à obtenir la combinaison qui minimise la distance entre le modèle et les points. Et voilà, c'est ça le machine learning. Ça consiste à développer un modèle en se servant d'un algorithme d'optimisation pour minimiser les erreurs entre le modèle et non-donné. Tout simplement. Et des modèles ont donc compte tout plein comme les modèles linières, les arts de décision ou encore les supportes vector machines. Chaque un venant avec son algorithme d'optimisation, la descende de gradients pour les modèles linières, l'algorithme carte pour les arts de décision ou encore la marge maximum pour les support vector machines. Maintenant, quand étîle, du diphe leur ning. Eh bien, le diplorning est un domaine du machine learning dans lequel, au lieu de développer un démodel que l'on vient de s'yter, on développe à la place ce qu'on appelle des réseaux de neurones artificiels. Alors le principe reste exactement le même. C'est à dire que l'on fourne et à la machine dédonné et él utiliser un algorithme d'optimisation pour ajuster le modèle.\n",
      "MoviePy - Writing audio in Folder_20230620_010206/media/mp3/segment_3.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      " d'elle assez donnée. Mais cette fois-ci notre modèle n'est pas une simple fonction du type F X et GAL AX plus B. Mais plutôt un réseau de fonction connecté les unes autres, un réseau de neurones. On verra dans un instant comment est-ce que ces réseaux sont construits, comment il fonctionne, mais ce qu'il faut savoir pour le moment c'est que plus ces réseaux sont profond, c'est-à-dire plus ils contiennent de fonction à l'intérieur, plus la machine est capable d'apprendre à réaliser des tâches complexes comme reconnaître des objets, identifiés une personne sur une photo, conduire une voiture, tout ce genre de choses. Voilà donc pourquoi on parle d'apprentissage profond ? C'est-à-dire de DIP, learning, lorsque longs développe des réseaux de neurones artificiels. Donc, retébient qu'en fait le DIP est un domaine du machine learning qui repose sur les mêmes fondations que celui-ci et que le machine learning est lui-même un domaine de l'intelligence artificielle. Voilà maintenant qu'on a bien posé les bases, je vous propose de commencer notre voyage dans le monde des réseaux de neurones artificiels. Pour bien comprendre le fonctionnement des réseaux de neurones artificiels, je voudrais revenir à l'origine de leur histoire. Vous racontez comment ils ont été inventés et quels furent leur évolution à travers le temps pour en arriver à la technologie que nous connaissons aujourd'hui. Cela va vous permettre de mieux comprendre et de mieux retner leur fonctionnement, mais également d'enrichir votre culture générale avec quelques anecdotes intéressantes sur l'intelligence artificielle. Les premiers réseaux de neurones ont donc été en\n",
      "MoviePy - Writing audio in Folder_20230620_010206/media/mp3/segment_4.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      " inventé en 1943 par deux mathématiciens et neuroscientifiques du nom de Warren MacLeod et Walter Pitz. Dans la rare articles scientifiques intitulées, aillogique le casque le S of the Ideas Immanant in nervous activity, il explique comment ils ont pu programer des neurones artificielles en s'aspirant du fonctionnement des neurones biologiques. Rappelant le, en biologie, les neurones sont des cellules excitables, connectées, les u, nos autres, et ayant pour rôle de transmettre des informations dans notre système nerveux. Chaque neurone est composée de plusieurs d'endrites, d'un corps cellulaire et d'un axon. Les d'endrites sont en quelque sorte les portes d'entrée d'un neurone. C'est à cet endroit au niveau de la Synapse que le neurone reçoit des signaux lui provenant des neurones qui le précede. Ces signaux peuvent être de type excitateur ou à l'inverse inhibiteur. Un peu comme si nous avions de signaux qui voulent plus un et d'autres qui voulent moins un. Lorsque la somme de ces signaux dépasse à certains soy, le neurone, s'active et produit alors un signal électrique. Ce signal circula le long de l'axone en direction des terminés on pourrait être envoyer à son tour vers d'autres neurones de notre système nerveux, neurone qui fonctionneront exactement de la même manière. Voilà en gros le fonctionnement de neurone. Ce que Warren MacClock et Walter Pitz ont essayé de faire, c'est de modéliser ce fonctionnement en considérant qu'un neurone pouvait être représenté par une fonction de transfert qui prend en entrée des signaux X et qui retournent une sortie hygrèque. À l'intérieur de cette fonction, on trouve deux grandes étapes. La première c'est une étape d'agrigation.\n",
      "MoviePy - Writing audio in Folder_20230620_010206/media/mp3/segment_5.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      " On fait la somme de toutes les entrées d'une rône, en multipliant au passage chaque entrée par un coefficient double v. Ce coefficient représente en fait l'activité synaptique. C'est-à-dire, le fait que le signal soit excitateur, auquel cas double v est v, plus 1 ou bien inhibiteur, auquel cas il v est v, moins 1. Dans cette phase d'agrigation, on obtient donc une expression de la forme double v1, plus double v2, plus double v3, et etc. Une fois cette étape réalisée, on passe à la phase d'activation. On regarde le résultat du calcul effectué précédemment, et si celui ci dépassent un certain seuil, en général 0, alors le neurone s'active et retourne une sortie hygrique et gala 1, sinon bien restas 0. Voilà donc comment Warren MacClock et Walter Pitz ont réussi à développer les premiers neurones artificielles, plus tard renommé Frashhold logic unit. Ce nom vient du fait qu'Alorigine, leur modèle n'était conçu que pour traiter des entrées logiques, qui valent soit 0 ou 1. Ils ont pu démontrer avec ce modèle, il était possible de reproduire certaines fonctions logiques, tel que la porte N, et la porte Or. Ils ont également démontré qu'en connectant plusieurs de ces fonctions, les unes autres, un petit peu à la manière des neurones de notre cerveau, alors il se réposible de résoudre n'importe quel problème de logique bouléenne. Forsément, suite à cette annonce, il u un engouement démesuré pour l'intelligence artificielle. Certaines personnes pensaient même quand quelques années nous saurions capables de développer des intelligences artificielles, capables de remplacer complètement les êtres humains. Bien sûr, il n'en fure rien. Car même ça, c'est pas possible.\n",
      "MoviePy - Writing audio in Folder_20230620_010206/media/mp3/segment_6.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      " Si ce modèle pose les bases de ce qu'est encore aujourd'hui, le diplôrenning, il contient un certain nombre de failles, notamment le fait qu'il ne dispose pas, d'algorithme d'apprentissage, et qu'il faut donc trouver nous-mêmes les valeurs des paramètres du bleuvet, s'il ont des ir sans servir pour des applications du monde réel. Résimant, une quinzaine d'années plus tard en 1957, un psychologue américain trouve à comment améliorer ce modèle, en proposant le premier algorithme d'apprentissage de l'histoire du diplôrenning. Ce monsieur, vous en avez peut-être déjà entendu parler, il s'agit de franc, rosonneblatt, l'inventeur du perceptron. Le modèle du perceptron ressemble en fait de très près à celui que nous venons d'étudier. Il s'agit d'un neuronartifical qui s'active lorsque la somme pondrait de ses entrées, dépassant certains soy, en général 0. Mais avec ça, le perceptron dispose également d'un algorithme d'apprentissage, lui permettant de trouver les valeurs de ses paramètres, doublever, afin d'obtenir les sorties hygrèques qui nous conviennent. Pour développer cet algorithme, franc, rosonneblatt, c'est inspiré de la théorie de Hab. Cette théorie, suggère que lorsque de neurones biologiques sont exités conjointement, alors il renforce leur lien cinaptique. C'est-à-dire qu'il renforce les connexions qu'il les uneisse. En neurosciences, c'est ce qu'on appelle la place d'icité cinaptique, et c'est ce qui permet à notre cerveau de construire sa mémoire, d'apprendre une nouvelle chose ou encore de faire de nouvelles associations. Donc, à partir de cette idée,\n",
      "MoviePy - Writing audio in Folder_20230620_010206/media/mp3/segment_7.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      " franque rosonne blade a développé un algorithme d'apprentissage qui consiste à entraîner un neurone artificiel sur des données de référence X in grec pour que celui-ci renforce ses paramètres d'oublévé à chaque fois qu'une entrée X est activée en même temps que la sortie in grec présente dans ses données. Pour ça il a imaginé la formule suivante dans laquelle les paramètres d'oublévés sont mis à jour en calculant la différence entre la sortie de référence et la sortie produite par le neurone et en multipliant cette différence par la valeur de chaque entre X ainsi que par un part d'apprentissage positif. De cette manière si notre neurone produit une sortie différente de celle qu'il est censée produire par exemple si nous sorts in grec, il y a le 0 alors qu'on voudrait avoir in grec et gale un. Alors notre formule nous donnera doublevé et gale doublevé plus alpha X. Donc pour les entrées X qui veulent un, le coefficient doublevé se verra augmenter d'un petit pas alpha il sera renforcé pour reprendre les termes de la théorie de Habe. Ce qui provoquera une augmentation de la fonction doublevé a Xin plus doublevé de X2 et qui rapprochera donc notre neurone de son seuil d'activation. Au silontant que l'on sera en dessous de ce seuil c'est-à-dire aussi longtemps que le neurone produira une mauvaise sortie. Alors le coefficient doublevé continuera d'augmenter grâce à notre formule jusqu'au moment où il grec trouve votre i grec et à ce moment-là notre formule d'un sera doublevé et gale doublevé plus 0 ce qui fait que nos paramètres arrêtrons dévoluer. Et voilà c'est ainsi.\n",
      "MoviePy - Writing audio in Folder_20230620_010206/media/mp3/segment_8.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      " que Frank Rosenblatt a développé le premier algorithme d'apprentissage de l'histoire du diplône et. Suite à cette invention, il lutte à nouveau un argument démesuré pour l'intelligence artificielle. On pensait que grâce aux perceptrons, il serait possible de construire des machines capable de lire, de parler, de marcher, et même d'avoir une conscience. Le truc de fou quoi ? Mais tout cet argument s'effondera quelques années plus tard, lorsqu'on se rendit compte que ces promesses ne pourraient être tenus en partie parce que le perceptron est un modèle linéaire, comme on le verra dans un instant. On connu alors le premier rivaire de l'intelligence artificielle de 1974 à 1980, période durant laquelle il n'y a quasiment plus d'investisseurs pour financer les recherches en hiat. L'intelligence artificielle était sur le point de mourir. Heureusement, tout changer dans les années 80, lorsque Jeffrey Hinton a des pares du diplôneignes, déflopa le perceptron multicouche, le premier véritable réseau de neurones artificielles. L'intelligence artificielle était sur le point de mourir. Comme je vous l'ai dit à l'instant, le perceptron est en fait un modèle linéaire. En effet, si l'ontrace la représentation graphique de sa fonction d'agrégation f de xin x2 égal, double verre xin, plus double éveux de x2, on obtient alors une droite dont l'aclinaison dépend des paramètres. Tout bleu verre, et dont la position peut être modifiée, à la d'un petit parametre complémentaire qu'on appelle le billet. Alors avec cette droite, on peut faire des choses géniales comme, par exemple, séparer deux classes de point, puisque grâce à notre fonction d'activation, tout se\n",
      "MoviePy - Writing audio in Folder_20230620_010206/media/mp3/segment_9.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      " ce qui sera au-dessus de cette droite, donnera une sortie hygrèque, et tout ce qui sera en dessous, donnera hygrèque et gazéro. Le seul ennuie, c'est qu'une grande partie des phénomènes de notre univers ne sont pas des phénomènes linéaires. Et dans ces conditions, le percephetroin à l'usole n'est pas très utile. Mais rappele-les-vous l'idée de maque-lock et pits, en connectant ensemble plusieurs neurones, il est possible de résoudre des problèmes plus complexes avec un seul. Voyons donc ce qu'il se passe si l'on connecte par exemple trois perceptrons ensemble. Les deux premiers recevables chacun les entrés, X-in et X-2, ils font leur petit calcul en fonction de leur paramètres et retournent une sortie hygrèque, qu'ils envoient à leur tour vers le troisième perceptron qui va lui aussi faire ces petits calculs pour produire une sortie finale. Et bien, si on trace la représentation de graphique de la sortie finale, en fonction des entrés X-in X-2, on obtient cette fois-ci un modèle non-linéaire qui est bien plus intéressant. Avec cet exemple, vous avez là votre premier réseau de neurones artificielles. Trois neurones, répartis en deux couches, une couche d'entrée et une couche de sortie, c'est ce qu'on appelle un perceptron multicouche. Et découche et des neurones, bien vous pouvez en rajouter autant que vous voulez, vous pouvez en mettre deux, trois, quatre, d'ice. Pourquoi pas même sans? Plus vous en metrez, plus le résultat à la sortie sera complexe et intéressant. Cependant, une question subsiste. Comment entraîner un tel réseau de neurones pour qu'il fasse ce qu'on lui demande de faire? C'est-à-dire comment trouver les valeurs de tous ces paramètres doublev et b de façon\n",
      "MoviePy - Writing audio in Folder_20230620_010206/media/mp3/segment_10.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      " obtenir un bon modèle. Eh bien la solution est d'utiliser une technique appelée BAC, Propagation, qui consiste à déterminer comment la sortie du réseau varie en fonction des paramètres présent dans chaque couche du modèle. Pour ça, on calcul une chaîne de gradients indiquant comment la sortie varie en fonction de la dernière couche puis comment la dernière couche varie en fonction de l'avant dernière et de cette érae jusqu'à arriver à la toute première couche de notre réseau. C'est une BAC, Propagation une propagation vers la rire. Avec ces informations, c'est gradients, on peut alors mettre à jour les paramètres de chaque couche de telle sorte à ce qu'il minimise les reires entre la sortie du modèle et de la réponse attendu la fameuse valeur hygrèque trou. Et pour ça, on utilise une formulae très proche de celle de France en résumée pour développer et entraîner des réseaux de nos articles, on répète en boucle les quatre étapes suivantes. La première étape, c'est l'étape de four wands Propagation. On fait circuler les données de la première couche jusqu'à la dernière afin de produire une sortie hygrèque. La deuxième étape, c'est de calculer les reires entre cette sortie et la sortie de référence hygrèque trou que l'on désir avoir et pour ça, on utilise ce qu'on appelle une fonction cou ou bien coste function en anglais. Ensuite, la troisième étape, c'est celle de la BAC propagation. Au revoir.\n",
      "MoviePy - Writing audio in Folder_20230620_010206/media/mp3/segment_11.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      " comment cette fonction coûte, varie par rapport à chaque couche de notre modèle, en partant de la dernière et en remontant jusqu'à la toute première. Et pour finir, la 4e et la dernière étape c'est de couriger chaque paramètres du modèle grâce à l'algorithme, de la décembre de gradients, avant de reboucler vers la première étape celle de la forment de propagait jeune pour recommencer un cycle d'entrainement. Je sais que ça peut faire beaucoup d'informations, vu comme ça, mais surtout, ne vous en faites pas, on va vraiment voir tout ça en détail. Bien qu'à le moment dans les prochaines vidéos. Là, c'est juste une petite vidéo d'introduction et à ce propos, il est en definiste notre histoire sur les réseaux de neurones artificielles. Au fil du temps, le modèle du perceptron multicouche continue à dévoluer, notamment avec l'apparition de nouvelles fonctions d'activation, telle que la fonction logistique, la fonction tangente hyperbolic, ou encore la fonction rectifide l'inéarionite. Ces fonctions ont aujourd'hui totalement remplacé la fonction heavy-side, que l'on a vu jusqu'à présent, car elles offrent en fait de bien meilleures performances. Dans les années 90, on commence à développer les premières variantes du perceptron multicouche. Le célèbre Yenne Locke, inventa les premiers réseaux de neurones convolutifs, réseaux qui sont capables de reconnaître et de traiter des images. En introduzant au début de ces réseaux, des filtres mathématiques qu'on appelle convolution et pouline. On en parlera plus tard dans cette formation. C'était également durant ces années que l'on a vu apparaître les premiers réseaux de neurones récurrents qui sont encore une fois une variante du perceptron multicouche.\n",
      "MoviePy - Writing audio in Folder_20230620_010206/media/mp3/segment_12.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      " et qui permettent de traiter efficacement des problèmes de série temporale comme la lecture de texte ou encore la reconnaissance vocale. Alors vous allez peut-être vous demander, mais si tout ça existe déjà dans les années 90, pourquoi avoir attendu aussi longtemps avant de voir émerger les technologies qu'on a aujourd'hui ? Eh bien il y a deux grandes raisons à cela. La première, c'est que pour bien fonctionner un réseau de neurone doit être entraîné sur une très grosse quantité de données. Des passants parfois les millions voire les dizaines de millions de données. Alors dans les années 90, on ne dispose pas d'autant de données. On n'avait pas des millions voire des dizaines de millions de photos de chiens, de chas, de voitures, de piétons, toutes bien classées et bien répertoriers. Non, il a fallu attendre la rivée d'un internet, des smartphones et des objectes connectés, pour commencer à collecter de grosses quantités de données, d'image et de son, pourront être exploité pour le diplône ingre. Maintenant, la deuxième raison pour laquelle il a fallu attendre si longtemps avant de pouvoir réalement utiliser des réseaux de neurone, c'est parce que la puissance des ordinateurs des années 90, de le permettre tout simplement pas. Et oui, parce qu'on le verra dans cette série de vidéos, entraîné un réseau de neurone, ça demande pas mal de temps et pas mal de puissance. Et il a fallu attendre qu'on dispose d'exélan CPU et GPU pour enfin obtenir deux bons résultats. En fait, le diplône ingre n'a réellement pris son envol quand 2012, lors d'une compétition de vision par n'yateur, nommée. Imaginez, ou une équipe de chercheurs menée par Jeffrey Hinton, développe un réseau de neurone capable de reconnaître n'importe quel image,\n",
      "MoviePy - Writing audio in Folder_20230620_010206/media/mp3/segment_13.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      " une meilleure performance que tous les autres algorithmes de l'époque. Depuis ce jour, tout le monde ne parle plus que de machine learning et de diplôneings. On vente sans arrêt les mérites de ces technologies. En allant même parfois un peu trop loin. Par exemple, quand on dit que les réseaux de neurones sa fonctionne, comme le cerveau humain, c'est complètement faux. Aujourd'hui, on s'est pertinemment que le cerveau humain est beaucoup plus complexe et beaucoup plus sophistiqués que le modèle du perceptron ou le perceptron multicouche. Et comme le Dianne Lequin, comparait un réseau de neurones, un cerveau humain, c'est un petit peu comme comparait un avion à un oiseau. On s'est peut-être inspiré de ce qu'on a vu dans la nature, au tout début, pour faire les premiers croquis. Mais c'est pas pour autant que les avions vol en batant des ailes. Non, derrière tout ça, ce sont des mathématiques de la j'abrolinaire, du calcul différentiel, tout une mécanique bien huillée qu'on va justement découvrir ensemble dans cette formation. Et en parlant de ça, il est temps de conclure cette vidéo, en vous montrant le programme que l'on va suivre, tout au long de cette série. Bon, j'espère que vous avez apprécié l'histoire que je vous ai raconté sur l'origine et l'évolution du diplône-ing. Si c'est le cas, n'hésitez pas à partager cette vidéo et à la mettre en avant avec un petit pouce bleu, moi-même ça va m'aider à savoir si j'ai fait du bon boulot. Maintenant, il est temps de découvrir le programme de cette formation et vous allez voir, il suit tout naturellement l'histoire que j'ai pu vous raconter. Pour commencer, on va étudier ensemble le modèle du perceptron simple, comment écrire et comprendre ces formules mathématiques, comment calculer sa performance, gris de la jolie.\n",
      "MoviePy - Writing audio in Folder_20230620_010206/media/mp3/segment_14.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      " grâce à la fonction cool qu'on appelle erreur logistique et comment entraîner ce modèle avec une descente de grâtion. À ce stade, je vous montrerai comment programmer tout ça en utilisant uniquement des matrices ainsi que Nune Paille. Ensuite, nous verrons ce qu'il se passe lorsqu'on rajoute un deuxième perceptron à côté du premier, puis également à troisième, le tout formant notre premier réseau de Nune Rône. On verra alors comment programmer l'étape de 4-1 propagation, puissait-le de bac propagation. Et, chose très importante, je ferai à la main, tous les calculs et des demonstrations mathématiques qui menent aux formules des différents gradiens de la bac propagation. Je vous assure, ça va être ultra cool de faire ça, et je vais faire en sorte que tout le monde puisse comprendre ce que j'ai créé. Ensuite, au fur et à mesure des vidéos, on continuera d'ajouter des neurones et des couches d'un hor réseau, en complétant à chaque fois, le code qu'on aura écrit dans les vidéos précédentes. Vous verrez que, finalement, une fois qu'on a bien posé les bases, eh bien, on peut créer n'importe quelle réseau de neurones sans avoir à vraiment modifier son code, ce qui est plutôt cool. À partir de là, je vais on te réégalement comment améliorer vos réseaux avec d'autres fonctions d'activation comme la fonction tangentie perbolyque, rectifier l'ignaire unit, ou encore la fonction softmax. Et une fois qu'on aura fait tout ça, il sera en fintant de découvrir la libérer, tangent surflo, je vous expliquerai comment fonctionne ce frein moire, ce qu'on trouve à l'intérieur, et comment développer facilement des réseaux de neurones grâce à sa libérer qui est rasse. On commencera alors à faire de vrais projets comme des projets de vision par la vie.\n",
      "MoviePy - Writing audio in Folder_20230620_010206/media/mp3/segment_15.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      " en éateurs ou je vous vous montrerai comment développer des applications telles que celle que vous vous voyez à l'écran. Et ben voilà j'espère que ce programme vous plaît et vous pouvez me croire avec tout ça vous serait calé en diplône. Alors bien sûr il faudra quand même pratiquer derrière mais vous en faites pas je serai là pour vous aider et pour vous motiver. Car la fin de chaque vidéo je vous proposerai un exercice avec dans la vidéo suivante le couragez de cet exercice. Je vous assure que c'est ultra important pour vous de faire ces exercices mais aussi de trouver d'autres projets sur Kaggle ou sur d'autres sites internet. Et d'ailleurs à ce titre bien si vous avez des idées de projets, des envies particulières, si vous avez des objectifs et bien n'hésitez pas à les partager. Dans les commentaires comme ça je pourrais en prendre conscience et je pourrais m'adapter à ces objectifs à l'avenir avec des nouvelles vidéos faites pour vous. Comme vous le savez ça fait deux ans que je suis sur cette chaîne YouTube et je répond absolument à tous les commentaires. C'est comme ça ça me fait une trapliseur d'avoir cette connexion avec les gens. Et d'ailleurs si vous venez d'arriver, si vous découvrez la base à cette chaîne YouTube, n'hésitez pas à rejoindre notre serveur Discord pour rencontrer le reste de la communauté parce que je dois bien la vouser. Il y a des gens formidable dans cette communauté. Il y a des gens mais vraiment incroyable. Vraiment profitez-en à les sur le Discord, posez vos questions, développer votre réseau et au passage en profit juste pour remercier du fond du coeur tous ces gens vraiment en est avec des gens géniaux. Bref sinon vous pouvez également vous rendre sur mon site internet, machinelenniac.com sur lequel vous pouvez trouver beaucoup de contenus supplémentaires ainsi que des formations gratuites.\n",
      "MoviePy - Writing audio in Folder_20230620_010206/media/mp3/segment_16.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      " sur ma page typie, vous pouvez me soutenir avec un don de 5 euros, ça aide beaucoup le développement de la chaîne, les projets, le matériel et en bonus, je vous donne à chaque fois du contenu exclusive, des tutors, des leçons, des cours, des vidéos qui ne sont disponibles que sur typie. Donc merci à tous ceux qui me soutiennent, encore une fois, je les remercie. Et ben voilà, j'espère que cette vidéo vous aura plus, j'espère que vous êtes aussi motivé que moi dans cette nouvelle aventure, si c'est le cas, bien merci de vous abonner pour ne pas louper les prochaines vidéos, quand à moi je vous dis à très vite.\n"
     ]
    }
   ],
   "source": [
    "import moviepy.editor as mp\n",
    "from myfunctions import *\n",
    "\n",
    "mp4_directory = \"Folder_20230620_010206/media/mp4\"\n",
    "mp3_directory = \"Folder_20230620_010206/media/mp3\"\n",
    "txt_directory = \"Folder_20230620_010206/media/txt\"\n",
    "\n",
    "video_mp4 = f\"{mp4_directory}/video.mp4\"\n",
    "\n",
    "\n",
    "def cut_and_convert_to_mp3(filename, mp3_directory, txt_directory):\n",
    "    video = mp.VideoFileClip(filename)\n",
    "    duration = video.duration\n",
    "\n",
    "    # Calculate the number of 2-minute segments\n",
    "    num_segments = int(duration // 120) + 1\n",
    "\n",
    "    for i in range(num_segments):\n",
    "        # Set the start and end times for the segment\n",
    "        start_time = i * 120  # 2 minutes\n",
    "        end_time = min((i + 1) * 120, duration)  # 2 minutes or remainder of the video\n",
    "\n",
    "        # Extract the segment and convert to MP3\n",
    "        segment = video.subclip(start_time, end_time)\n",
    "        output_filename = f\"{mp3_directory}/segment_{i + 1}.mp3\"\n",
    "        segment.audio.write_audiofile(output_filename)\n",
    "\n",
    "        result = transcribe_mp3(mp3_directory, f\"segment_{i + 1}\")\n",
    "\n",
    "\n",
    "        txt_path = f\"{txt_directory}/output_{i + 1}.txt\" \n",
    "        # Open the file in write mode\n",
    "        with open(txt_path, 'w') as file:\n",
    "            # Write the data to the file\n",
    "            file.write(result['text'])\n",
    "\n",
    "    video.close()\n",
    "\n",
    "convert_to_mp3(video_mp4, mp4_directory, mp3_directory, txt_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "880.728"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from mutagen.mp3 import MP3\n",
    "from myfunctions import *\n",
    "\n",
    "audio_file = \"onlymp3.to -  -Bf8ESGFh-nI-192k-1687021754.mp3\"\n",
    "audio = MP3(audio_file)\n",
    "duration = audio.info.length\n",
    "\n",
    "audio = AudioSegment.from_file(audio_file)\n",
    "duration = len(audio) / 1000  # Convert to seconds\n",
    "\n",
    "# Calculate the number of 5-minute segments\n",
    "num_segments = int(duration // 300) + 1\n",
    "result_text = ''\n",
    "for i in range(num_segments):\n",
    "    # Set the start and end times for the segment\n",
    "    start_time = i * 300 * 1000  # 5 minutes (converted to milliseconds)\n",
    "    end_time = min((i + 1) * 300 * 1000, len(audio))  # 5 minutes or remainder of the audio\n",
    "\n",
    "    # Extract the segment\n",
    "    segment = audio[start_time:end_time]\n",
    "\n",
    "    # Set the output filename for the segment\n",
    "    output_filename = f\"segment_{i + 1}.mp3\"\n",
    "\n",
    "    # Save the segment as an MP3 file\n",
    "    segment.export(output_filename, format=\"mp3\")\n",
    "\n",
    "    # Transcribe the MP3 segment using a function called transcribe_mp3\n",
    "    result = transcribe_mp3(output_filename, f\"segment_{i + 1}\")\n",
    "\n",
    "    for segment in result['segments']:\n",
    "        start = round(float(segment['start']), 2)\n",
    "        end = round(float(segment['end']), 2)\n",
    "        text = segment['text']\n",
    "        print(f\"[{start} : {end}] : {text}\")\n",
    "\n",
    "    result_text += result['text'] + \" \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = [\"In the beginning stages of your data science project, you see dependency management tools like people or conda, maybe sufficient. However, as a data science project expands, the number of dependencies also increases. This can make it difficult to reproduce the project's environment when relying solely on people or conda. To address these challenges, poultry, an open source library provides a powerful tool for creating and maintaining Python projects. In this video, we will delve into the advantages of poultry and halalach, its key distinctions from PIP and conda, having a broad selection of packages, messet easier for developers to find the specific package that messwed their needs. Conda has a limited number of packages. Some packages, like SNS-grade, cannot be installed with conda, additionally, certain versions, such as pandas to log all, might not be available for installation through conda. While you can use PIP inside a conda virtual environment, conda cannot track dependencies installed with PIP, making dependency management challenging. PIP and poultry can install any packages from the Python package index and other repositories. Reducing the number of dependencies, simplifies the development process. Conda provides full environment isolation, managing both Python packages and system-level dependencies. This can result in larger package size compared to other package managers. To illustrate this, we will install pandas in a new conda environment, and list all packages installed in this environment. We can see that a lot of these packages are not pandas dependencies. PIP and poultry install only the dependencies required by a package. On installing packages and their dependencies pre-up disk space and prevents unnecessary clutter. PIP removes only the specified package, not its dependencies potentially leading to the accumulation of unused dependencies over time. Conda removes the package and some of its dependencies, but not our dependencies. Poetry removes a package and all of its dependencies. Dependency files specifies the exact versions, all versions ranges of required packages. To save dependencies in a conda environment, you need to manually write them to a file. Let's assume that we are using pandas version 1.3.1. If a new user tries to reproduce environment, when the latest versions of pandas is 1.5.3, pandas 1.5.3 will be installed instead. If the code may rely on syntax, specific to pandas versions 1.3.1, and the syntax has changed in versions 1.5.3, running the code with the new version of pandas would introduce bugs. The same problem can occur with PIP. Imagine the requirements docteersc file looks like this. Now let's create an activate a virtual environment and install dependencies from the requirements doctersc file. You can see that pandas 2.0 is installed. Of course, you can ping down the version by freezing them in a requirements doctersc file. However, this mess the code environment lacks flexible and potentially harder to maintain in the long run. Any changes to the dependencies would require manual modification in the requirements doctersc file, which can be time consuming an error prompt. Poetry automatically updates the PIP project docter file where the installing a package. This flexible versioning approach ensures that your project can adapt to newer releases with our manual adjustment. The Poetry docter file store the precise version numbers for each package and its dependencies. This guarantee consistency in the installed packages. Here we can see that pandas 1.5.3 is installed instead of pandas 2.0. By separating the dependencies, you can clearly distinguish between the packages required for development purposes from the packages needed for the production environment. This ensures that the production environment contains only the necessary packages for running the application, producing the risk of conflicts. Pandas doesn't inherently support separate delivery.  dependencies for different environments. Both a workaround involves creating two environment files, one for the production environment and one for development. The development file contains both production and development dependencies. Deep also does and directly support separate dependencies, but a similar approach can be used with separate requirement files. To install the production dependencies type, clip install requirements.txt. To install most types of requirements, type, clip install requirements, depth.txt. Poetry simplifies managing dependencies by supporting groups within one file. To illustrate this, we will add number pie and pandas to the main group and pie tests and pre-commit to the development group. We can see that both groups of dependencies are added to the pie project.time file. This allows you to keep track of all dependencies in a single place. To install only production dependencies, type, poetry install only men. To install most development and production dependencies, type, poetry install. Obbeding dependencies is essential to benefit from workplaces and new features introduced in newer package versions. Condor allows you to update only a specified package. For example, if we want to update pandas poles and cycle run, we need to run the command, condor update for each package. Afterwards, you need to manually update the environment.yammo file. To keep its sync with the updated dependencies, people also only allow you to update a specified package and requires you to manually update the requirements.timest file. With poetry, you can use the update command to upgrade our packages, specified in the priproject.time file. This action automatically updates the poetry.lof file ensuring consistency between the package specifications and the log file. dependency conflicts occurred when packages required by a project have conflicting dependencies. Feed install packages so casually, which means it installs each package one by one. For example, suppose you install pandas to the old doc2, which requires number pie greater than or equal to one doc20.c. Later, you install number pie, equal to one doc20.c.timest file. Even though this will create dependency conflicts, tip will proceed to update the version of number pie. Condor uses a S80 server to explore our combinations of package versions and dependencies for instance. If an existing package has a specific constraint for its dependency, and the package you want to install does a mid-dry requirement. Condor want immediately raise an error, instead it will deligion to leave such for compatible versions of other required packages and their dependencies. Only reporting an error if no suitable solution is found. While this approach enhances the chances of finding a solution, it can be computationally intensive, particularly when dealing with extensive environments. By focusing on the direct dependencies of the project, poultry's deterministic resolver narrow-scout the search space. It evaluates the specified constraints and immediately identifies any conflicts. To demonstrate this, let's install a specific version of C-borne, that requires match plot-leap version 3.1 or HIGHGLE. If we attempt to install a version of match plot-leap, that falls outside of this required range, C-borne will promptly flow an error. This immediate feedback allows developers to address the problem early in the development process. For example, in this case, we can relax a requirement for C-borne to enable the installation of a specific version of match plot-leap. In summary, poultry provides several advantages over PIP and Condor. Firstly, poultry provides access to a wide range of packages. Secondly, poultry installs only the necessary dependencies, thirdly poultry simplifies the removal of packages and their dependencies. Lastly, poultry efficiently resolves dependencies and addresses any conflicts promptly. Efficient dependency management, adjust the initial step in constructing a maintainable data science project. To ensure long-term maintainable data science project,  it is crucial to avoid hard coding. In this next video, you will learn how to use a configuration file to eliminate hard coding in your next Python project. \"\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"At Microsoft, we have been on a quest to advance AI beyond existing techniques, by taking a more holistic, human-centric approach to learning and understanding. As Chief Technology Officer of Azure AI Cognitive Services, I have been working with a team of amazing scientists and engineers to turn this quest into a reality. In my role, I enjoy a unique perspective in viewing the relationship among three attributes of human cognition: monolingual text (X), audio or visual sensory signals, (Y) and multilingual (Z). At the intersection of all three, there's magic-what we call XYZ-code as illustrated in Figure 1-a joint representation to create more powerful AI that can speak, hear, see, and understand humans better. We believe XYZ-code will enable us to fulfill our long-term vision: cross-domain transfer learning, spanning modalities and languages. The goal is to have pretrained models that can jointly learn representations to support a broad range of downstream AI tasks, much in the way humans do today. Over the past five years, we have achieved human performance on benchmarks in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning. These five breakthroughs provided us with strong signals toward our more ambitious aspiration to produce a leap in AI capabilities, achieving multisensory and multilingual learning that is closer in line with how humans learn and understand. I believe the joint XYZ-code is a foundational component of this aspiration, if grounded with external knowledge sources in the downstream AI tasks.\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Folder_20230703_220807/media/txt/output.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Folder_20230703_220807/media/txt/output.txt'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_path = \"Folder_20230703_220807/media/txt/output.txt\"\n",
    "file = open(txt_path, \"r\")\n",
    "# Read the contents of the file\n",
    "file_contents = file.read()\n",
    "# Close the file\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_contents = [\"  When I was young, there was an amazing publication called The Whole Earth Catalog, which was one of the Bibles of my generation. It was created by a fellow named Stuart Brand, not far from here in Menlo Park, and he brought it to life with his poetic touch. This was in the late 60s before personal computers and desktop publishing, so it was all made with typewriters, scissors, and Polarite cameras. It was sort of like Google and paperback form 35 years before Google came along. It was idealistic overflowing with neat tools and great notions. Stuart and his team put out several issues of the whole Earth Catalog, and then when it had run its course, they put out a final issue. It was the mid-1970s and I was your age. On the back cover of their final issue was a photograph of an early morning country road. The kind you might find yourself hitchhiking on if you were so adventurous. Beneath it were the words, stay hungry, stay foolish. It was their farewell message as they signed off. Stay hungry, stay foolish. And I have always wished that for myself. And now, as you graduate to begin a new, I wish that for you. Stay hungry, stay foolish. Thank you all very much. \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from myfunctions.my_summarization_functions import *\n",
    "dico = list_to_dict(file_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity</th>\n",
       "      <th>Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Product</td>\n",
       "      <td>[The Whole Earth Catalog, Bibles, personal com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quantity</td>\n",
       "      <td>[one, age]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Person</td>\n",
       "      <td>[Stuart Brand, Stuart]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Location</td>\n",
       "      <td>[Menlo Park, country road]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Skill</td>\n",
       "      <td>[life, desktop publishing, idealistic]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DateTime</td>\n",
       "      <td>[60s, 35 years, 1970s, early morning, now]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Organization</td>\n",
       "      <td>[Google, Google]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Entity                                             Values\n",
       "0       Product  [The Whole Earth Catalog, Bibles, personal com...\n",
       "1      Quantity                                         [one, age]\n",
       "2        Person                             [Stuart Brand, Stuart]\n",
       "3      Location                         [Menlo Park, country road]\n",
       "4         Skill             [life, desktop publishing, idealistic]\n",
       "5      DateTime         [60s, 35 years, 1970s, early morning, now]\n",
       "6  Organization                                   [Google, Google]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(dico.items(), columns=['Entity', 'Values'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_whisper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
